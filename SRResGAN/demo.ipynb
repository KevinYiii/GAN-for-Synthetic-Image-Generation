{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T07:49:24.224651800Z",
     "start_time": "2023-12-24T07:49:24.199716800Z"
    }
   },
   "id": "d3ac9f712902d2cf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de7339adecfd6af6"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "transform_hr = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "transform_lr = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "class FaceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform_hr, transform_lr):\n",
    "        self.root_dir = os.path.join(root_dir, 'raw')  # Update this line to include 'raw'\n",
    "        self.transform_hr = transform_hr\n",
    "        self.transform_lr = transform_lr\n",
    "        # Ensure that we only list files (this filters out subdirectories if there are any)\n",
    "        self.images = [f for f in os.listdir(self.root_dir) if os.path.isfile(os.path.join(self.root_dir, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.images[idx])\n",
    "        image_hr = Image.open(img_name).convert('RGB')  # Convert to RGB to avoid issues with RGBA or grayscale images\n",
    "        image_lr = image_hr.copy()\n",
    "        image_hr = self.transform_hr(image_hr)\n",
    "        image_lr = self.transform_lr(image_lr)\n",
    "        return image_lr, image_hr\n",
    "\n",
    "dataset = FaceDataset('../dataset/face_imgs', transform_hr, transform_lr)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T07:49:24.338405900Z",
     "start_time": "2023-12-24T07:49:24.239609300Z"
    }
   },
   "id": "c8f0c22131433e0f"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class SRResNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_blocks=8):\n",
    "        super(SRResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4)\n",
    "        self.prelu = nn.PReLU()\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.residuals = nn.Sequential(*[ResidualBlock(64) for _ in range(num_blocks)])\n",
    "        \n",
    "        # Another conv layer after residual blocks\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Upsampling layer\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.conv3 = nn.Conv2d(64, in_channels, kernel_size=9, stride=1, padding=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.prelu(self.conv1(x))\n",
    "        res = self.residuals(out1)\n",
    "        out2 = self.bn2(self.conv2(res)) + out1\n",
    "        out3 = self.upsample(out2)\n",
    "        out = self.conv3(out3)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(512, 1024, kernel_size=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(1024, 1, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T07:49:24.372315400Z",
     "start_time": "2023-12-24T07:49:24.348379800Z"
    }
   },
   "id": "549d53f14904acdd"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "srresnet = SRResNet()\n",
    "discriminator = Discriminator()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T07:49:24.415203Z",
     "start_time": "2023-12-24T07:49:24.371319400Z"
    }
   },
   "id": "c8c3a2e6b8e8333b"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()  # 二元交叉熵损失\n",
    "optimizer_srresnet = optim.Adam(srresnet.parameters(), lr=0.001)\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T07:49:24.430161800Z",
     "start_time": "2023-12-24T07:49:24.416198100Z"
    }
   },
   "id": "cf00c2763f9ff1c1"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCUDA_LAUNCH_BLOCKING\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      3\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m \u001B[43msrresnet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m discriminator\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      7\u001B[0m sample_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./samples\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\pt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001B[0m, in \u001B[0;36mModule.to\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1156\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1157\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m   1158\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[1;32m-> 1160\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\pt\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 810\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    814\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    815\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    820\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    821\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\pt\\lib\\site-packages\\torch\\nn\\modules\\module.py:833\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    829\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[0;32m    830\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[0;32m    831\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[0;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 833\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    834\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[0;32m    835\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\pt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1158\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m   1155\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[0;32m   1156\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1157\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m-> 1158\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "srresnet.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "sample_dir = './samples'\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 5  # 根据需要调整\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images_lr, images_hr) in enumerate(dataloader):\n",
    "        images_lr = images_lr.to(device)\n",
    "        images_hr = images_hr.to(device)\n",
    "\n",
    "        # 真实和假的标签\n",
    "        real_labels = torch.ones(images_hr.size(0), 1).to(device)\n",
    "        fake_labels = torch.zeros(images_hr.size(0), 1).to(device)\n",
    "\n",
    "        # 训练判别器\n",
    "        optimizer_discriminator.zero_grad()\n",
    "        real_outputs = discriminator(images_hr).squeeze().unsqueeze(1)\n",
    "        d_loss_real = criterion(real_outputs, real_labels)\n",
    "        fake_images = srresnet(images_lr)\n",
    "        fake_outputs = discriminator(fake_images.detach()).squeeze().unsqueeze(1)\n",
    "        d_loss_fake = criterion(fake_outputs, fake_labels)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_discriminator.step()\n",
    "\n",
    "        # 训练生成器 (SRResNet)\n",
    "        optimizer_srresnet.zero_grad()\n",
    "        fake_images = srresnet(images_lr)\n",
    "        outputs = discriminator(fake_images).squeeze().unsqueeze(1)\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        g_loss.backward()\n",
    "        optimizer_srresnet.step()\n",
    "\n",
    "        # 打印损失信息\n",
    "        if (i + 1) % 53 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(dataloader)}], d_loss: {d_loss.item()}, g_loss: {g_loss.item()}')\n",
    "\n",
    "        # 每个epoch结束时保存一些生成的样本\n",
    "        if (i + 1) % 53 == 0:\n",
    "            fake_images = fake_images.reshape(fake_images.size(0), 3, 512, 512)\n",
    "            save_image(fake_images, os.path.join(sample_dir, f'sample_epoch_{epoch}_{i}.png'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T07:49:24.584748100Z",
     "start_time": "2023-12-24T07:49:24.436145400Z"
    }
   },
   "id": "b50e476c216f1576"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T07:49:24.586742600Z",
     "start_time": "2023-12-24T07:49:24.585744900Z"
    }
   },
   "id": "c8868b0cb815d50c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T07:49:24.596984900Z",
     "start_time": "2023-12-24T07:49:24.588736900Z"
    }
   },
   "id": "46706ee795f01c5d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
